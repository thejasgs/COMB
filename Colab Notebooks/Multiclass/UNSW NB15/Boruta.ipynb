{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Boruta.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aAnyL10uJP4H","colab_type":"code","colab":{}},"source":["from __future__ import print_function, division\n","import numpy as np\n","import scipy as sp\n","from sklearn.utils import check_random_state, check_X_y\n","from sklearn.base import TransformerMixin, BaseEstimator\n","\n","\n","class BorutaPy(BaseEstimator, TransformerMixin):\n","    \"\"\"\n","    Improved Python implementation of the Boruta R package.\n","    The improvements of this implementation include:\n","    - Faster run times:\n","        Thanks to scikit-learn's fast implementation of the ensemble methods.\n","    - Scikit-learn like interface:\n","        Use BorutaPy just like any other scikit learner: fit, fit_transform and\n","        transform are all implemented in a similar fashion.\n","    - Modularity:\n","        Any ensemble method could be used: random forest, extra trees\n","        classifier, even gradient boosted trees.\n","    - Two step correction:\n","        The original Boruta code corrects for multiple testing in an overly\n","        conservative way. In this implementation, the Benjamini Hochberg FDR is\n","        used to correct in each iteration across active features. This means\n","        only those features are included in the correction which are still in\n","        the selection process. Following this, each that passed goes through a\n","        regular Bonferroni correction to check for the repeated testing over\n","        the iterations.\n","    - Percentile:\n","        Instead of using the max values of the shadow features the user can\n","        specify which percentile to use. This gives a finer control over this\n","        crucial parameter. For more info, please read about the perc parameter.\n","    - Automatic tree number:\n","        Setting the n_estimator to 'auto' will calculate the number of trees\n","        in each itartion based on the number of features under investigation.\n","        This way more trees are used when the training data has many feautres\n","        and less when most of the features have been rejected.\n","    - Ranking of features:\n","        After fitting BorutaPy it provides the user with ranking of features.\n","        Confirmed ones are 1, Tentatives are 2, and the rejected are ranked\n","        starting from 3, based on their feautre importance history through\n","        the iterations.\n","    We highly recommend using pruned trees with a depth between 3-7.\n","    For more, see the docs of these functions, and the examples below.\n","    Original code and method by: Miron B Kursa, https://m2.icm.edu.pl/boruta/\n","    Boruta is an all relevant feature selection method, while most other are\n","    minimal optimal; this means it tries to find all features carrying\n","    information usable for prediction, rather than finding a possibly compact\n","    subset of features on which some classifier has a minimal error.\n","    Why bother with all relevant feature selection?\n","    When you try to understand the phenomenon that made your data, you should\n","    care about all factors that contribute to it, not just the bluntest signs\n","    of it in context of your methodology (yes, minimal optimal set of features\n","    by definition depends on your classifier choice).\n","    Parameters\n","    ----------\n","    estimator : object\n","        A supervised learning estimator, with a 'fit' method that returns the\n","        feature_importances_ attribute. Important features must correspond to\n","        high absolute values in the feature_importances_.\n","    n_estimators : int or string, default = 1000\n","        If int sets the number of estimators in the chosen ensemble method.\n","        If 'auto' this is determined automatically based on the size of the\n","        dataset. The other parameters of the used estimators need to be set\n","        with initialisation.\n","    perc : int, default = 100\n","        Instead of the max we use the percentile defined by the user, to pick\n","        our threshold for comparison between shadow and real features. The max\n","        tend to be too stringent. This provides a finer control over this. The\n","        lower perc is the more false positives will be picked as relevant but\n","        also the less relevant features will be left out. The usual trade-off.\n","        The default is essentially the vanilla Boruta corresponding to the max.\n","    alpha : float, default = 0.05\n","        Level at which the corrected p-values will get rejected in both\n","        correction steps.\n","    two_step : Boolean, default = True\n","        If you want to use the original implementation of Boruta with Bonferroni\n","        correction only set this to False.\n","    max_iter : int, default = 100\n","        The number of maximum iterations to perform.\n","    random_state : int, RandomState instance or None; default=None\n","        If int, random_state is the seed used by the random number generator;\n","        If RandomState instance, random_state is the random number generator;\n","        If None, the random number generator is the RandomState instance used\n","        by `np.random`.\n","    verbose : int, default=0\n","        Controls verbosity of output:\n","        - 0: no output\n","        - 1: displays iteration number\n","        - 2: which features have been selected already\n","    Attributes\n","    ----------\n","    n_features_ : int\n","        The number of selected features.\n","    support_ : array of shape [n_features]\n","        The mask of selected features - only confirmed ones are True.\n","    support_weak_ : array of shape [n_features]\n","        The mask of selected tentative features, which haven't gained enough\n","        support during the max_iter number of iterations..\n","    ranking_ : array of shape [n_features]\n","        The feature ranking, such that ``ranking_[i]`` corresponds to the\n","        ranking position of the i-th feature. Selected (i.e., estimated\n","        best) features are assigned rank 1 and tentative features are assigned\n","        rank 2.\n","    Examples\n","    --------\n","    \n","    import pandas as pd\n","    from sklearn.ensemble import RandomForestClassifier\n","    from boruta import BorutaPy\n","    \n","    # load X and y\n","    # NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n","    X = pd.read_csv('examples/test_X.csv', index_col=0).values\n","    y = pd.read_csv('examples/test_y.csv', header=None, index_col=0).values\n","    y = y.ravel()\n","    \n","    # define random forest classifier, with utilising all cores and\n","    # sampling in proportion to y labels\n","    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n","    \n","    # define Boruta feature selection method\n","    feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n","    \n","    # find all relevant features - 5 features should be selected\n","    feat_selector.fit(X, y)\n","    \n","    # check selected features - first 5 features are selected\n","    feat_selector.support_\n","    \n","    # check ranking of features\n","    feat_selector.ranking_\n","    \n","    # call transform() on X to filter it down to selected features\n","    X_filtered = feat_selector.transform(X)\n","    References\n","    ----------\n","    [1] Kursa M., Rudnicki W., \"Feature Selection with the Boruta Package\"\n","        Journal of Statistical Software, Vol. 36, Issue 11, Sep 2010\n","    \"\"\"\n","\n","    def __init__(self, estimator, n_estimators=1000, perc=100, alpha=0.05,\n","                 two_step=True, max_iter=100, random_state=None, verbose=0):\n","        self.estimator = estimator\n","        self.n_estimators = n_estimators\n","        self.perc = perc\n","        self.alpha = alpha\n","        self.two_step = two_step\n","        self.max_iter = max_iter\n","        self.random_state = random_state\n","        self.verbose = verbose\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fits the Boruta feature selection with the provided estimator.\n","        Parameters\n","        ----------\n","        X : array-like, shape = [n_samples, n_features]\n","            The training input samples.\n","        y : array-like, shape = [n_samples]\n","            The target values.\n","        \"\"\"\n","\n","        return self._fit(X, y)\n","\n","    def transform(self, X, weak=False):\n","        \"\"\"\n","        Reduces the input X to the features selected by Boruta.\n","        Parameters\n","        ----------\n","        X : array-like, shape = [n_samples, n_features]\n","            The training input samples.\n","        weak: boolean, default = False\n","            If set to true, the tentative features are also used to reduce X.\n","        Returns\n","        -------\n","        X : array-like, shape = [n_samples, n_features_]\n","            The input matrix X's columns are reduced to the features which were\n","            selected by Boruta.\n","        \"\"\"\n","\n","        return self._transform(X, weak)\n","\n","    def fit_transform(self, X, y, weak=False):\n","        \"\"\"\n","        Fits Boruta, then reduces the input X to the selected features.\n","        Parameters\n","        ----------\n","        X : array-like, shape = [n_samples, n_features]\n","            The training input samples.\n","        y : array-like, shape = [n_samples]\n","            The target values.\n","        weak: boolean, default = False\n","            If set to true, the tentative features are also used to reduce X.\n","        Returns\n","        -------\n","        X : array-like, shape = [n_samples, n_features_]\n","            The input matrix X's columns are reduced to the features which were\n","            selected by Boruta.\n","        \"\"\"\n","\n","        self._fit(X, y)\n","        return self._transform(X, weak)\n","\n","    def _fit(self, X, y):\n","        # check input params\n","        self._check_params(X, y)\n","        self.random_state = check_random_state(self.random_state)\n","        # setup variables for Boruta\n","        n_sample, n_feat = X.shape\n","        _iter = 1\n","        # holds the decision about each feature:\n","        # 0  - default state = tentative in original code\n","        # 1  - accepted in original code\n","        # -1 - rejected in original code\n","        dec_reg = np.zeros(n_feat, dtype=np.int)\n","        # counts how many times a given feature was more important than\n","        # the best of the shadow features\n","        hit_reg = np.zeros(n_feat, dtype=np.int)\n","        # these record the history of the iterations\n","        imp_history = np.zeros(n_feat, dtype=np.float)\n","        sha_max_history = []\n","\n","        # set n_estimators\n","        if self.n_estimators != 'auto':\n","            self.estimator.set_params(n_estimators=self.n_estimators)\n","\n","        # main feature selection loop\n","        while np.any(dec_reg == 0) and _iter < self.max_iter:\n","            # find optimal number of trees and depth\n","            if self.n_estimators == 'auto':\n","                # number of features that aren't rejected\n","                not_rejected = np.where(dec_reg >= 0)[0].shape[0]\n","                n_tree = self._get_tree_num(not_rejected)\n","                self.estimator.set_params(n_estimators=n_tree)\n","\n","            # make sure we start with a new tree in each iteration\n","            self.estimator.set_params(random_state=self.random_state)\n","\n","            # add shadow attributes, shuffle them and train estimator, get imps\n","            cur_imp = self._add_shadows_get_imps(X, y, dec_reg)\n","\n","            # get the threshold of shadow importances we will use for rejection\n","            imp_sha_max = np.percentile(cur_imp[1], self.perc)\n","\n","            # record importance history\n","            sha_max_history.append(imp_sha_max)\n","            imp_history = np.vstack((imp_history, cur_imp[0]))\n","\n","            # register which feature is more imp than the max of shadows\n","            hit_reg = self._assign_hits(hit_reg, cur_imp, imp_sha_max)\n","\n","            # based on hit_reg we check if a feature is doing better than\n","            # expected by chance\n","            dec_reg = self._do_tests(dec_reg, hit_reg, _iter)\n","\n","            # print out confirmed features\n","            if self.verbose > 0 and _iter < self.max_iter:\n","                self._print_results(dec_reg, _iter, 0)\n","            if _iter < self.max_iter:\n","                _iter += 1\n","\n","        # we automatically apply R package's rough fix for tentative ones\n","        confirmed = np.where(dec_reg == 1)[0]\n","        tentative = np.where(dec_reg == 0)[0]\n","        # ignore the first row of zeros\n","        tentative_median = np.median(imp_history[1:, tentative], axis=0)\n","        # which tentative to keep\n","        tentative_confirmed = np.where(tentative_median\n","                                       > np.median(sha_max_history))[0]\n","        tentative = tentative[tentative_confirmed]\n","\n","        # basic result variables\n","        self.n_features_ = confirmed.shape[0]\n","        self.support_ = np.zeros(n_feat, dtype=np.bool)\n","        self.support_[confirmed] = 1\n","        self.support_weak_ = np.zeros(n_feat, dtype=np.bool)\n","        self.support_weak_[tentative] = 1\n","\n","        # ranking, confirmed variables are rank 1\n","        self.ranking_ = np.ones(n_feat, dtype=np.int)\n","        # tentative variables are rank 2\n","        self.ranking_[tentative] = 2\n","        # selected = confirmed and tentative\n","        selected = np.hstack((confirmed, tentative))\n","        # all rejected features are sorted by importance history\n","        not_selected = np.setdiff1d(np.arange(n_feat), selected)\n","        # large importance values should rank higher = lower ranks -> *(-1)\n","        imp_history_rejected = imp_history[1:, not_selected] * -1\n","\n","        # update rank for not_selected features\n","        if not_selected.shape[0] > 0:\n","                # calculate ranks in each iteration, then median of ranks across feats\n","                iter_ranks = self._nanrankdata(imp_history_rejected, axis=1)\n","                rank_medians = np.nanmedian(iter_ranks, axis=0)\n","                ranks = self._nanrankdata(rank_medians, axis=0)\n","\n","                # set smallest rank to 3 if there are tentative feats\n","                if tentative.shape[0] > 0:\n","                    ranks = ranks - np.min(ranks) + 3\n","                else:\n","                    # and 2 otherwise\n","                    ranks = ranks - np.min(ranks) + 2\n","                self.ranking_[not_selected] = ranks\n","        else:\n","            # all are selected, thus we set feature supports to True\n","            self.support_ = np.ones(n_feat, dtype=np.bool)\n","\n","        # notify user\n","        if self.verbose > 0:\n","            self._print_results(dec_reg, _iter, 1)\n","        return self\n","\n","    def _transform(self, X, weak=False):\n","        # sanity check\n","        try:\n","            self.ranking_\n","        except AttributeError:\n","            raise ValueError('You need to call the fit(X, y) method first.')\n","\n","        if weak:\n","            X = X[:, self.support_ + self.support_weak_]\n","        else:\n","            X = X[:, self.support_]\n","        return X\n","\n","    def _get_tree_num(self, n_feat):\n","        depth = self.estimator.get_params()['max_depth']\n","        if depth == None:\n","            depth = 10\n","        # how many times a feature should be considered on average\n","        f_repr = 100\n","        # n_feat * 2 because the training matrix is extended with n shadow features\n","        multi = ((n_feat * 2) / (np.sqrt(n_feat * 2) * depth))\n","        n_estimators = int(multi * f_repr)\n","        return n_estimators\n","\n","    def _get_imp(self, X, y):\n","        try:\n","            self.estimator.fit(X, y)\n","        except Exception as e:\n","            raise ValueError('Please check your X and y variable. The provided'\n","                             'estimator cannot be fitted to your data.\\n' + str(e))\n","        try:\n","            imp = self.estimator.feature_importances_\n","        except Exception:\n","            raise ValueError('Only methods with feature_importance_ attribute '\n","                             'are currently supported in BorutaPy.')\n","        return imp\n","\n","    def _get_shuffle(self, seq):\n","        self.random_state.shuffle(seq)\n","        return seq\n","\n","    def _add_shadows_get_imps(self, X, y, dec_reg):\n","        # find features that are tentative still\n","        x_cur_ind = np.where(dec_reg >= 0)[0]\n","        x_cur = np.copy(X[:, x_cur_ind])\n","        x_cur_w = x_cur.shape[1]\n","        # deep copy the matrix for the shadow matrix\n","        x_sha = np.copy(x_cur)\n","        # make sure there's at least 5 columns in the shadow matrix for\n","        while (x_sha.shape[1] < 5):\n","            x_sha = np.hstack((x_sha, x_sha))\n","        # shuffle xSha\n","        x_sha = np.apply_along_axis(self._get_shuffle, 0, x_sha)\n","        # get importance of the merged matrix\n","        imp = self._get_imp(np.hstack((x_cur, x_sha)), y)\n","        # separate importances of real and shadow features\n","        imp_sha = imp[x_cur_w:]\n","        imp_real = np.zeros(X.shape[1])\n","        imp_real[:] = np.nan\n","        imp_real[x_cur_ind] = imp[:x_cur_w]\n","        return imp_real, imp_sha\n","\n","    def _assign_hits(self, hit_reg, cur_imp, imp_sha_max):\n","        # register hits for features that did better than the best of shadows\n","        cur_imp_no_nan = cur_imp[0]\n","        cur_imp_no_nan[np.isnan(cur_imp_no_nan)] = 0\n","        hits = np.where(cur_imp_no_nan > imp_sha_max)[0]\n","        hit_reg[hits] += 1\n","        return hit_reg\n","\n","    def _do_tests(self, dec_reg, hit_reg, _iter):\n","        active_features = np.where(dec_reg >= 0)[0]\n","        hits = hit_reg[active_features]\n","        # get uncorrected p values based on hit_reg\n","        to_accept_ps = sp.stats.binom.sf(hits - 1, _iter, .5).flatten()\n","        to_reject_ps = sp.stats.binom.cdf(hits, _iter, .5).flatten()\n","\n","        if self.two_step:\n","            # two step multicor process\n","            # first we correct for testing several features in each round using FDR\n","            to_accept = self._fdrcorrection(to_accept_ps, alpha=self.alpha)[0]\n","            to_reject = self._fdrcorrection(to_reject_ps, alpha=self.alpha)[0]\n","\n","            # second we correct for testing the same feature over and over again\n","            # using bonferroni\n","            to_accept2 = to_accept_ps <= self.alpha / float(_iter)\n","            to_reject2 = to_reject_ps <= self.alpha / float(_iter)\n","\n","            # combine the two multi corrections, and get indexes\n","            to_accept *= to_accept2\n","            to_reject *= to_reject2\n","        else:\n","            # as in th original Boruta, we simply do bonferroni correction\n","            # with the total n_feat in each iteration\n","            to_accept = to_accept_ps <= self.alpha / float(len(dec_reg))\n","            to_reject = to_reject_ps <= self.alpha / float(len(dec_reg))\n","\n","        # find features which are 0 and have been rejected or accepted\n","        to_accept = np.where((dec_reg[active_features] == 0) * to_accept)[0]\n","        to_reject = np.where((dec_reg[active_features] == 0) * to_reject)[0]\n","\n","        # updating dec_reg\n","        dec_reg[active_features[to_accept]] = 1\n","        dec_reg[active_features[to_reject]] = -1\n","        return dec_reg\n","\n","    def _fdrcorrection(self, pvals, alpha=0.05):\n","        \"\"\"\n","        Benjamini/Hochberg p-value correction for false discovery rate, from\n","        statsmodels package. Included here for decoupling dependency on statsmodels.\n","        Parameters\n","        ----------\n","        pvals : array_like\n","            set of p-values of the individual tests.\n","        alpha : float\n","            error rate\n","        Returns\n","        -------\n","        rejected : array, bool\n","            True if a hypothesis is rejected, False if not\n","        pvalue-corrected : array\n","            pvalues adjusted for multiple hypothesis testing to limit FDR\n","        \"\"\"\n","        pvals = np.asarray(pvals)\n","        pvals_sortind = np.argsort(pvals)\n","        pvals_sorted = np.take(pvals, pvals_sortind)\n","        nobs = len(pvals_sorted)\n","        ecdffactor = np.arange(1, nobs + 1) / float(nobs)\n","\n","        reject = pvals_sorted <= ecdffactor * alpha\n","        if reject.any():\n","            rejectmax = max(np.nonzero(reject)[0])\n","            reject[:rejectmax] = True\n","\n","        pvals_corrected_raw = pvals_sorted / ecdffactor\n","        pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n","        pvals_corrected[pvals_corrected > 1] = 1\n","        # reorder p-values and rejection mask to original order of pvals\n","        pvals_corrected_ = np.empty_like(pvals_corrected)\n","        pvals_corrected_[pvals_sortind] = pvals_corrected\n","        reject_ = np.empty_like(reject)\n","        reject_[pvals_sortind] = reject\n","        return reject_, pvals_corrected_\n","\n","    def _nanrankdata(self, X, axis=1):\n","        \"\"\"\n","        Replaces bottleneck's nanrankdata with scipy and numpy alternative.\n","        \"\"\"\n","        ranks = sp.stats.mstats.rankdata(X, axis=axis)\n","        ranks[np.isnan(X)] = np.nan\n","        return ranks\n","\n","    def _check_params(self, X, y):\n","        \"\"\"\n","        Check hyperparameters as well as X and y before proceeding with fit.\n","        \"\"\"\n","        # check X and y are consistent len, X is Array and y is column\n","        X, y = check_X_y(X, y)\n","        if self.perc <= 0 or self.perc > 100:\n","            raise ValueError('The percentile should be between 0 and 100.')\n","\n","        if self.alpha <= 0 or self.alpha > 1:\n","            raise ValueError('Alpha should be between 0 and 1.')\n","\n","    def _print_results(self, dec_reg, _iter, flag):\n","        n_iter = str(_iter) + ' / ' + str(self.max_iter)\n","        n_confirmed = np.where(dec_reg == 1)[0].shape[0]\n","        n_rejected = np.where(dec_reg == -1)[0].shape[0]\n","        cols = ['Iteration: ', 'Confirmed: ', 'Tentative: ', 'Rejected: ']\n","\n","        # still in feature selection\n","        if flag == 0:\n","            n_tentative = np.where(dec_reg == 0)[0].shape[0]\n","            content = map(str, [n_iter, n_confirmed, n_tentative, n_rejected])\n","            if self.verbose == 1:\n","                output = cols[0] + n_iter\n","            elif self.verbose > 1:\n","                output = '\\n'.join([x[0] + '\\t' + x[1] for x in zip(cols, content)])\n","\n","        # Boruta finished running and tentatives have been filtered\n","        else:\n","            n_tentative = np.sum(self.support_weak_)\n","            content = map(str, [n_iter, n_confirmed, n_tentative, n_rejected])\n","            result = '\\n'.join([x[0] + '\\t' + x[1] for x in zip(cols, content)])\n","            output = \"\\n\\nBorutaPy finished running.\\n\\n\" + result\n","        print(output)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Us7qbsV6IrDy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"outputId":"ee63aa92-3a58-4f4e-bf95-ec98462797aa","executionInfo":{"status":"ok","timestamp":1562095327055,"user_tz":240,"elapsed":18928,"user":{"displayName":"Daniel Jimenez","photoUrl":"","userId":"03897706589372651705"}}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/Research/'  #change dir to your project folder\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nF10uixkIj78","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2befb50a-08e0-412b-9ccb-5f8d24d3bd64","executionInfo":{"status":"ok","timestamp":1562097256270,"user_tz":240,"elapsed":1734428,"user":{"displayName":"Daniel Jimenez","photoUrl":"","userId":"03897706589372651705"}}},"source":["import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","# load X and y\n","# NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n","X = pd.read_csv(root_path+'Data/Features.csv', index_col=0).values\n","y = pd.read_csv(root_path+'Data/Target.csv', index_col=0).values\n","y = y.ravel()\n","\n","# define random forest classifier, with utilising all cores and\n","# sampling in proportion to y labels\n","rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n","\n","# define Boruta feature selection method\n","feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n","\n","# find all relevant features - 5 features should be selected\n","feat_selector.fit(X, y)\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Iteration: \t1 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t2 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t3 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t4 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t5 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t6 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t7 / 100\n","Confirmed: \t0\n","Tentative: \t43\n","Rejected: \t0\n","Iteration: \t8 / 100\n","Confirmed: \t40\n","Tentative: \t3\n","Rejected: \t0\n","Iteration: \t9 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t10 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t11 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t12 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t13 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t14 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t15 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t16 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t17 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t18 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t19 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t20 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t21 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t22 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t23 / 100\n","Confirmed: \t40\n","Tentative: \t1\n","Rejected: \t2\n","Iteration: \t24 / 100\n","Confirmed: \t40\n","Tentative: \t0\n","Rejected: \t3\n","\n","\n","BorutaPy finished running.\n","\n","Iteration: \t25 / 100\n","Confirmed: \t40\n","Tentative: \t0\n","Rejected: \t3\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["BorutaPy(alpha=0.05,\n","         estimator=RandomForestClassifier(bootstrap=True,\n","                                          class_weight='balanced',\n","                                          criterion='gini', max_depth=5,\n","                                          max_features='auto',\n","                                          max_leaf_nodes=None,\n","                                          min_impurity_decrease=0.0,\n","                                          min_impurity_split=None,\n","                                          min_samples_leaf=1,\n","                                          min_samples_split=2,\n","                                          min_weight_fraction_leaf=0.0,\n","                                          n_estimators=181, n_jobs=-1,\n","                                          oob_score=False,\n","                                          random_state=<mtrand.RandomState object at 0x7f8068d86ee8>,\n","                                          verbose=0, warm_start=False),\n","         max_iter=100, n_estimators='auto', perc=100,\n","         random_state=<mtrand.RandomState object at 0x7f8068d86ee8>,\n","         two_step=True, verbose=2)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"h2GT-48kJEw3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"a1209b6e-b85f-40b3-8a36-e4ca9b6585b5","executionInfo":{"status":"ok","timestamp":1562097256277,"user_tz":240,"elapsed":1679999,"user":{"displayName":"Daniel Jimenez","photoUrl":"","userId":"03897706589372651705"}}},"source":["# check selected features - first 5 features are selected\n","feat_selector.support_\n","\n"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True, False,  True,  True, False, False,\n","        True,  True,  True,  True,  True,  True,  True])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"9_j1YO4dJF7n","colab_type":"code","colab":{}},"source":["# check ranking of features\n","feat_selector.ranking_\n","\n","# call transform() on X to filter it down to selected features\n","X_filtered = feat_selector.transform(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o9vhoAelNN1v","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","x_train, x_test, y_train, y_test = train_test_split(X_filtered, y, random_state=11)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"srHNP8L8NbKG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"9aa3d7a5-57f9-41af-c029-60b73e19ef13","executionInfo":{"status":"ok","timestamp":1562097432291,"user_tz":240,"elapsed":56029,"user":{"displayName":"Daniel Jimenez","photoUrl":"","userId":"03897706589372651705"}}},"source":["rf.fit(x_train, y_train)\n","y_pred = rf.predict(x_test)\n","print(classification_report(y_test,y_pred, digits=4))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["                precision    recall  f1-score   support\n","\n","      Analysis     0.4192    0.1761    0.2480       619\n","      Backdoor     0.0678    0.3774    0.1149       567\n","           DoS     0.3303    0.5060    0.3997      4075\n","      Exploits     0.8029    0.4295    0.5596     11236\n","       Fuzzers     0.8310    0.7664    0.7974      6127\n","       Generic     0.9999    0.9739    0.9867     53886\n","Reconnaissance     0.7705    0.7569    0.7636      3385\n","     Shellcode     0.1785    0.8402    0.2945       388\n","         Worms     0.0235    0.8421    0.0456        38\n","\n","      accuracy                         0.8380     80321\n","     macro avg     0.4915    0.6298    0.4678     80321\n","  weighted avg     0.9003    0.8380    0.8577     80321\n","\n"],"name":"stdout"}]}]}